
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="hu">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Backpropagation többrétegű hálózatnál (MLP) &#8212; ANN  dokumentáció</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/translations.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Tárgymutató" href="../genindex.html" />
    <link rel="search" title="Keresés" href="../search.html" />
    <link rel="prev" title="Tanulás" href="learning.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="backpropagation-tobbretegu-halozatnal-mlp">
<h1>Backpropagation többrétegű hálózatnál (MLP)<a class="headerlink" href="#backpropagation-tobbretegu-halozatnal-mlp" title="Hivatkozás erre a fejezetcímre">¶</a></h1>
<div class="section" id="egy-processzalo-elem-szigmoid-kimeneti-nemlinearitassal">
<h2>Egy processzáló elem szigmoid kimeneti nemlinearitással<a class="headerlink" href="#egy-processzalo-elem-szigmoid-kimeneti-nemlinearitassal" title="Hivatkozás erre a fejezetcímre">¶</a></h2>
<blockquote>
<div><a class="reference internal image-reference" href="../_images/nonlinear_sigmoid.png"><img alt="Generation" class="align-center" src="../_images/nonlinear_sigmoid.png" style="width: 350px; height: 200px;" /></a>
</div></blockquote>
<p>A hálózat felépítését az fenti ábrán láthatjuk. A súlymódosítást az LMS algoritmussal végezzük, a hibát a teljes hálózat kimenetén értelmezzük.</p>
<div class="math">
\[\varepsilon (k) = d(k) - y(k) = d(k) - sgm(s(k)) = d(k) - sgm(w^T (k)x(k))\]</div>
<p>A pillanatnyi gradiens tehát:</p>
<div class="math">
\[\dfrac{\partial \varepsilon ^2}{\partial w} = 2\varepsilon(-sgm'(s))x\]</div>
<p>ahol sgm»(s) a kimeneti nemlinearitás deriváltját jelöli.</p>
<p>A logisztikus függvény, ill. deriváltja:</p>
<div class="math">
\[y = sgm(s) = \dfrac{1}{1+e^{-s}},       y' = sgm'(s) = y(1-y)\]</div>
</div>
<div class="section" id="a-backpropagation-algoritmus">
<h2>A backpropagation algoritmus<a class="headerlink" href="#a-backpropagation-algoritmus" title="Hivatkozás erre a fejezetcímre">¶</a></h2>
<blockquote>
<div><a class="reference internal image-reference" href="../_images/backpropagation.png"><img alt="Generation" class="align-center" src="../_images/backpropagation.png" style="width: 350px; height: 200px;" /></a>
</div></blockquote>
<p>A többrétegű hálózatok felépítése a fenti ábrán követhető. Az ábra egy két aktív régeteg tartalmazó hálózatot mutat, amelyben az első aktív rétegben - a rejetett rétegben - három, a második aktív rétegben - jelen esetben a kimeneti rétegben - két processzáló elem található. A hálózat tehát egy többrétegű előrecsatolt hálózat(FeedForward). A súlyvektorok meghatározása gradiens alapú ellenőrzött tanuló eljárással, tehát összetartozó (x,y) tanító párok felhasználásával történik.</p>
<p>A tanító eljárás bemutatásához vezessük be a következő jelöléseket: legyen <em>l</em> a réteg index, <em>i</em> a rétegen belüli processzáló elem (PE) indexe, <em>j</em> a PE bemeneteit megkülönböztető index - ez lesz egyben a megfelelő súlyvektor komponenseinek indexe -  ez lesz egyben a megfelelő súlyvektor komponenseinek indexe is - és <em>k</em> a diszkrét időindex.(pl. PE_i^(l) az l-edk réteg i-edik processzáló elemét jelöli, w_i^(l) (k) pedig az <em>l</em>-edik réteg, <em>i</em>-edik prcesszáló elemének súlyvektorát a <em>k</em>-adik időpillanatban és w_ij^(l) (k) ugyanezen súlyvektornak a j-edik komponensét jelöli.)</p>
<p>A hálózat tanítását két (aktív) rétegű hálózaton mutatjuk be. A kimenet hiba:</p>
<div class="math">
\[\varepsilon ^2 = \varepsilon_1^2 + \varepsilon_2^2 = (y_1 - d_1)^2 + (y_2 - d_2)^2\]</div>
<p>A súly módosításhoz a megfelelő pillanatnyi gradienseket kell kiszámítani.</p>
<div class="math">
\[\dfrac{\partial \varepsilon ^2}{\partial w_{ij}^{(l)}}\]</div>
<p>A <em>kimeneti szinten</em> (l=2) a pillanyatni gradiens:</p>
<div class="math">
\[dfrac{\partial \varepsilon ^2}{\partial w_{ij}^{(2)}} = -2\varepsilon_isgm'(s_i^{(2)})x_j^{(2)} = -2\delta_i^{(2)}x_j^{(2)}\]</div>
<p>A súlymódosítás tehát</p>
<div class="math">
\[w_i^{(2)}(k+1) = w_i^{(2)}(k) + 2\mu\varepsilon_i(k)sgm'(s_i^{(2)})x^{(2)}(k) = w_i^{(2)} + 2\mu\delta_i^{(2)}(k)x^{(2)}(k)\]</div>
<p>A rejtett réteg processzáló elemeinél a fenti összefüggések közvetlenül nem alkalmazhatók, mivel nem ismerjük az egyes processzáló elemek kimenetén fellépő hibát. A lánc szabály alkalmazásával azonban a deriváltak itt is meg tudjuk határozni, hiszen a rejtett réteg processzáló elemeinek súlytényezői befolyásolják ezen processzáló elemek lineáris (s) és nemlineáris (y) kimeneteit, továbbá ezen kimeneteken keresztül a későbbi rétegek kimeneteit is. Tehát a parciális deriváltak lépésenként számíthatók.</p>
<div class="math">
\[\dfrac{\partial \varepsilon ^2}{\partial w_{ij}^{(1)}} = \dfrac{\partial \varepsilon ^2}{\partial s_i^{(1)}}\dfrac{\partial s_i^{(1)}}{\partial w_{ij}^{(1)}}\]</div>
<p>így a súlymódosítás:</p>
<div class="math">
\[w_i^{(1)}(k+1)=w_i^{(1)}(k)+2\mu\delta_i^{(1)}(k)x^{(1)}(k)\]</div>
<p>ahol</p>
<div class="math">
\[\delta_i^{(1)} = (\delta_i^{(2)}w_{1i}^{(2)}+\delta_2^{(2)}w_{2i}^{(2)})sgm'(s_i{(1)})\]</div>
<p>âz ún. „visszaterjesztett hiba”. a súlymódosítás tehát itt is az LMS algoritmussal formailag megegyező eljárással történik, a hiba helyén azonban súlyzott, „visszaterjesztett hiba” szerpel. A súlyozó együtthatók megegyeznek az adott hálózat-részben az előrecsatolásánál szereplő súlytényezőkkel. A súlymódosítás ennek alapján tetszőleges rétegre megadható:</p>
<div class="math">
\[w_i^{(l)}(k+1)=w_i^{(l)}+2\mu\delta_i^{(l)}(k)x^{(l)}(k)\]</div>
<p>Ha az l-edik réteg összes processzáló elemének súlyvektorait egy W^{(l)} mátrixba fogjuk össze, ahol tehát a mátrix i-edik sora az i-edik processzáló elem súlyvektora, akkor az l-edik réteg összes súlyvektorának módosítása tömören az alábbi formában adható meg:</p>
<div class="math">
\[W^{(l)}(k+1) = W^{(l)}(k)+2\mu\delta^{(l)}x^{(l)^T}(k)\]</div>
<p>Itt a delta^{(l)} a visszaterjesztett „hibákból” képzett oszlopvektor.</p>
<p>Az előbbiekben bemutattuk a backpropagation hálózat felépítését, működését. A hálózat elvi alapjainak ismerete azonban még nem elegendő ahhoz, hoy e hálüzatokat hatékonyan alkalmazni is tudjuk különböző gyakorlati feladatok megoldására. A hálózattal kapcsolatos elméleti eredmények ugyanis a gyakorlati alkalmazással összefüggő kérdésekre általában nem adnak választ.</p>
<dl class="docutils">
<dt>Ilyen kérdések lehetnek pl.:</dt>
<dd><ul class="first last simple">
<li>mekkora (hány réteg, rétegenként hány processzáló elem) hálózatot válasszunk</li>
<li>hogyan válasszuk meg a tanulási aránytényező, mu értékét</li>
<li>milyen kezdeti értékeket állítsunk be</li>
<li>hogyan válasszuk meg a tanító és a tesztelő minta készletet</li>
<li>hogyan használjuk fel a tanító pontokat, milyen gyakorisággal módosítsuk a hálózat súlyait</li>
<li>meddig tanítsuk a hálózatot, stb?</li>
</ul>
</dd>
</dl>
<p>Áltlában az előbb feltett kérdéskre is csak tapasztalati válaszok adhatók. Ez az oka annak, hogy a neurális hálózatok sikeres alkalmazásához jelenleg meglehetősen sok tapasztalat szükséges, amit csak úgy szerezhetünk meg, ha számos, jellegében eltérő feladat megoldására vállalkozunk.</p>
<p><strong>A hálózat méretének megválasztása</strong>. Az elméleti eredmények szerint legalább háromrétegű - két tanítható réteggel rendelkező - hálózatra van szükség. A réteg számának növelése azonban megkönnyítheti a feladat megoldását, illetve rétegenként kevesebb processzáló elem felhasználása is elegendőnek bizonyulhat.</p>
<p>A redundancia csökkentése azt jelenti, hogy megprobáljuk megkeresni a felesleges súlyokat, processzáló elemeket, esetleg rétegeket, majd ezeket a hálózatból kimetszve a processzáló elemeket, esetleg rétegeket, majd ezeket a hálózatból kimetszve a maradék, egyszerűsített hálózattal oldjuk meg a feladatot. Felesleges súlyoknak, processzáló elemeknek, esetleg rétegeknek azok a hálózatelemek tekinthetők, melyek kihagyásával a feladat megoldható, tehát amelyek vagy nem vesznek részt a kimenet elóállításában vagy amelyek szerepét más hálózatelem is betöltheti, így a hálózat képességeinek redukciója nélkül elhagyhatók.</p>
<dl class="docutils">
<dt>A hálózatok bizonyos kimetszéssel történő csökkentésére alkalmazott módszereket két csoportba sorolhatjuk:</dt>
<dd><ul class="first last simple">
<li>Az egyik csoportba tartozó eljárások a kimeneti hiba egyes súlyok szerinti érzékenységének becslésén alapulnak</li>
<li>A másik csoportba tartozó módszereknél a kritériumfüggvényhez egy újabb ún. büntető tagot adunk</li>
</ul>
</dd>
</dl>
<p>Az érzékenység becslés alapján dolgozó eljárások a hibafelület súlyok szerinti Taylor-soros közelítéséből indulnak ki</p>
<p>A büntető tagot alkalmazó módszereknél a tanulás során a hálózat súlyainak csökkentésére is törekszünk. A kritériumfüggvény:</p>
<div class="math">
\[C_r(w) = C(w) + \lambda\sum_{i,j}|w_{ij}|\]</div>
<p><strong>A tanulási aránytényző, mu megválasztására</strong> sincs jelenleg egyértelműen javasolható egyszerű módszer. A legtöbb esetben mu értékét tapasztalati úton határozzák meg általános tendeciá felhasználásával. A gyorsabb kezdeti konvergencia és a minimumhely megfelelő pontosságú megközelítése érhető változó, lépésfüggő mu alkalmazásával. Ebben az esetben valamely kezdeti, viszonylag nagy értékből kiindulva valahány lépésenként csökkentjuük mu -t.</p>
<div class="math">
\[\sum_{k=1}^{\infty}\mu(k) = \infty\]</div>
<p>Ebben az összefüggésben k nem feltétlenül a lépésindexet jelöli, mu csökkentésére rendszerint csak néhány lépésenként kerül sor.</p>
<p>A tanulás konvergenciájának sebessége növelhető adaptív mu választással is. Ha a hiba nem csökken, akkor mu értéke túl nagy, csökkenteni kell. Ezzel szemben, ha több egymást követő tanító lépés során a hiba folyamatosan csökken, akkor lehetséges, hogy túl óvatosan választottuk meg mu-t, valószínű nagyobb érték mellett is biztosított a hiba csökkenése, érdemes tehát nagyobb mu-vel megkísérelni a tanítást.</p>
<p><strong>A kezdeti súlyok beállítására</strong> - jelenleg szintén nincs matematikailag megfogalmazható összefüggés. A kezdeti súlyvektor a hibafelületen a kezdeti pont helyzetét határozza meg, így minnél messzebb van ez a pont a megoldás helyétől, annál lassabban tanul a hálózat. A véletlen kezdeti értékek a szimmetriák elkerülését biztosíthatják, megakadályozva, hogy különböző neuronok hasonló leképezést valósítsanak meg és így nemkívánt redundancia jelenjen meg a hálózatban.</p>
<p><strong>A tanító lépések számának meghatározása</strong>. Általában előre nem határozható meg a hálózat hibájának alakulása a tanító lépések függvényében, így azt sem lehet megmondani, hogy megfelelő kis hiba eléréséhe hány tanító lépés szükséges. A hálózat „jóságának” kiértékelése amúgy is számos elméleti ésé gyakorlati problémát vet fel.</p>
<p><strong>A tanító pontok felhasználása</strong>. További lehetőségeket jelent, hogy a súlymódosításokat pontonként, vagy kötegelt (batch) eljárás szerint, csak a teljes tanító készlet (epoch) felhasználása után végezzük. Ezek az eljárások a súlykorrekció gyakoriságában térnek el. A batch eljárásnál a teljes tanító készlet összes mintáját ráadva a hálózatra minden esetben kiszámítjuk a hibát. A mintánkénti tanítás mintegy zajt visz a rendszerbe.</p>
<p>További kérdés, hogy egyáltalán <strong>hogyan minősitsünk egy hálózatot</strong>. Adott tanító készlet mellett szükségünk van egy ún. minősítő, kiértékelő mintakészletre (validation set) is. A hálózat tanítására csaka  tanítókészlet mintáit használjuk, míg az adott számú tanító lépésben átesett hálózat minősítése a minősítő készletre adott válaszok alapján lehetséges. Amennyiben csak a tanító pontokra adott válaszok alapján értékelünk, túltaníthatjuk a hálózatot.</p>
<p>Túltanítás (overtraining) akkor lép fel, ha a tanító készlet mintáira már nagyon kis hibájú válaszokat kapunk, miközben a kiértékelő készletre egyre nagyobb hibával válaszol a hálózat. Ez azért következhet be, mert a hálózat válaszai túlzottan illeszkednek a véges számú tanító pont által megszabott lekéezéshez, miközben a közbenső válaszok jelentősen eltérhetnek a megfelelő kivánt válaszoktól.</p>
<blockquote>
<div><a class="reference internal image-reference" href="../_images/overtraining.png"><img alt="Generation" class="align-center" src="../_images/overtraining.png" style="width: 350px; height: 200px;" /></a>
</div></blockquote>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="learning.html" title="előző fejezet">Tanulás</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Gyorskeresés</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Ok" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Szilvasi_Peter.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/pages/backpropagation.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>